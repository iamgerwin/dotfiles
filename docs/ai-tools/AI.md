# AI Coding Workflow Guidelines

## ğŸ¯ Core Principles
- **Mental alignment**: Always seek clarification, never assumeâ€”ask even at 90% certainty
- **Incremental commits**: Granular, trackable progress with conventional commits
- **No AI attribution**: Never mention Claude/AI/co-authored in commits, PRs, or comments
- **No auto-posting**: Never comment on ClickUp unless explicitly instructed
- **Stay in Smart Zone**: Keep context <40% capacity (avoid "dumb zone" degradation)
- **Frequent Intentional Compaction**: Compress context to markdown after every phase
- **No Vibes Coding**: Research â†’ Plan â†’ Implement (RPI). Never skip planning
- **Trajectory Awareness**: Reset conversations showing failure patterns (yell-correct loops)
- **Specify Persona**: Identify what's the basic characteristic of agent, Senior Developer ? Backend? Frontend? UI UX? this will boost quality

## ğŸ—ï¸ Branching & Commit Strategy

### For ClickUp Feature/Bugfix Tasks

1. Branch from `latest develop`: [commit-type]/[clickup-id]-[short-description]
   - commit-type: `feat`, `fix`, `docs`, `style`, `refactor`, `test`, `chore`, `cherrypick`
   - Example: `feat/CU-1234-add-user-profile-api`

2. Create **TWO PR sets**:
   - **PR #1**: Feature branch â†’ `develop`
   - **PR #2**: Cherry-pick commits â†’ `main` (resolve conflicts, exact commits only)

## âœ… Pre-Implementation Checks

- [ ] Document only what's necessary
- [ ] No destructive changes/migrations
- [ ] Focus ONLY on required files
- [ ] Defensive coding - preserve existing behavior
- [ ] Mimic existing patterns or document best-practice deviations
- [ ] Always ask when in doubt (90% rule)
- [ ] Context size: <40% window capacity
- [ ] Compaction: Current state â†’ tagged markdown
- [ ] Trajectory: No failure patterns in history
- [ ] Onboarding: Dynamic research (not static docs)
- [ ] Sub-agent scope: Discovery only, succinct returns

## ğŸ”¬ RPI Workflow (Research â†’ Plan â†’ Implement)

Store findings: `docs/plans/[ticket-id]-[short-title].md`

### RESEARCH Phase
```
File: docs/plans/[ticket]-research.md

â–¡ Objective codebase scan (files, line numbers, flows)
â–¡ On-demand vertical slices (no stale docs)
â–¡ Compress truth from code itself
â–¡ Sub-agents for scoped discovery only
â–¡ Search for impacted files proactively
â–¡ Zero tolerance for assumptions/hallucinations
â–¡ Clarify requirements for mental alignment
```

### PLAN Phase
```
File: docs/plans/[ticket]-plan.md

â–¡ Explicit steps + file paths + code snippets
â–¡ Post-change test instructions
â–¡ Human review REQUIRED (90% rule)
â–¡ Sweet spot: Detailed for agents, readable for humans
â–¡ Code snippets show exact changes
â–¡ Test validation after each step
```

### IMPLEMENT Phase
```
â–¡ Execute plan only - no improvisation
â–¡ Compact prior state before next iteration
â–¡ Fresh context window with plan only
â–¡ Focus on files outlined in plan
```

## ğŸ’ Coding Standards

- âœ… Use enums (avoid magic strings/numbers)
- âœ… Eliminate code smells
- âœ… New enums â†’ respective directories
- âœ… Match existing codebase style
- âœ… Document intentional best-practice improvements

## ğŸš« Anti-Slop Rules

```
âŒ NEVER: "Vibe coding" without RPI
âŒ NEVER: Skip human plan review
âŒ NEVER: >40% context (dumb zone)
âŒ NEVER: Static docs (they lie most)
âŒ NEVER: Anthropomorphize sub-agents (roles)
âœ… ALWAYS: Compact before reset
âœ… ALWAYS: Code snippets in plans
âœ… ALWAYS: Mental alignment via plan review
```

## ğŸ“‹ Final Deliverables Checklist

### 1. Create Pull Request Summary

File: `docs/pullrequests/[ticket-id]-[short-title].md`

**Format:**

# [Ticket Title]

## RPI Summary
```
Research: docs/plans/[ticket]-research.md
Plan: docs/plans/[ticket]-plan.md â† HUMAN APPROVED
Implementation: Executed per plan
Context Compactions: [X] iterations
```

## PR Links
- **Develop PR**: [link]
- **Main Cherry-pick PR**: [link]

## Status
- [ ] Code Review

## Description
[Clear implementation summary]

## Files Changed
[List key files with purpose]

## Technical Details
[Bullet points on approach, decisions, tests]

## Mental Alignment Notes
[Key decisions from plan review, human sign-offs]

## Reviewers
@reviewer1 @reviewer2 [ClickUp task assignees]

### 2. Critical Features

Create: `docs/[feature-name]/[implementation|fix|improvement].md`

### 3. Git Operations

- [ ] Professional PR titles/descriptions
- [ ] Push both PRs (develop + main cherry-pick)
- [ ] Verify all commits included correctly
- [ ] Attach RPI summary/threads to PR for transparency

## ğŸš€ Enhanced RPI Workflow

```
1. RESEARCH â†’ docs/plans/[ticket]-research.md (compress truth)
2. PLAN â†’ docs/plans/[ticket]-plan.md (HUMAN REVIEW)
3. IMPLEMENT â†’ Fresh context w/ plan only
4. COMPACT â†’ docs/plans/[ticket]-compaction-[n].md
5. REPEAT â†’ Stay in smart zone (<40%)
6. MENTAL ALIGNMENT â†’ Plan review > code review
```

## ğŸ§  Mental Alignment Strategy

**Why it matters**: Keep the entire engineering team synchronized on how the codebase is evolving and whyâ€”not just catching bugs, but understanding intent.

**How to achieve it**:
- Human-in-loop: Review research/plans iteratively; peers approve before implementation
- Leverage compression: Detailed-but-readable plans hit the sweet spot (reliable execution + skimmable)
- Cultural shift: Share plans in PRs for transparency; prevents seniors cleaning up juniors' slop
- Catch early: One bad research line cascades to 100 bad code linesâ€”review hierarchy matters

**Key insight**: Read plans instead of 1000+ lines of code. Leaders stay informed without drowning in diffs.

## âš ï¸ "Don't Outsource the Thinking" & "This Isn't Magic"

**Core warnings**:
- AI cannot replace human judgmentâ€”it amplifies whatever you provide (garbage in = garbage out)
- No perfect prompt exists; success demands human review of research and plans
- Bad inputs cascade catastrophically: Misunderstanding flow â†’ flawed plan â†’ wrong execution
- Watch for tools spewing unvetted markdownâ€”stay in the loop

**Practical application**:
- Humans drive highest-leverage steps (research validation, plan approval)
- Shift effort from code reading to oversight
- Build intuition through repsâ€”get it wrong repeatedly
- Without discipline, teams rift: juniors vibe-code slop, seniors burn out fixing it

## ğŸ“š Context Engineering Checklist

```
â–¡ Context size: <40% window capacity
â–¡ Compaction: Current state â†’ tagged markdown
â–¡ Trajectory: No failure patterns in conversation
â–¡ Onboarding: Dynamic research (not stale static docs)
â–¡ Sub-agents: Scoped discovery, succinct returns
â–¡ Research: Grounded in code, not assumptions
â–¡ Plans: Include code snippets, test steps
â–¡ Implementation: Execute plan only
â–¡ Review: Human sign-off before agent work
```

## ğŸ“ Learning & Iteration

- Pick one tool (Claude/Cursor) and get reps
- You will get RPI scope wrong repeatedlyâ€”that's normal
- Too much compaction = missed context; too little = dumb zone
- Find your team's sweet spot through practice
- Avoid minmaxing across multiple tools

## ğŸš€ Workflow Summary

1. Branch from develop â†’ Implement incrementally (RPI)
2. Research/Plan â†’ `docs/plans/[ticket].md` (human review)
3. Code â†’ Match standards, enums, no destruction
4. `docs/pullrequests/[ticket].md` â†’ Review summary with RPI
5. PR #1: develop | PR #2: main (cherry-pick)
6. ASK for clarification â†’ Mental alignment first
7. Compact context â†’ Prepare for next iteration

**Remember**: Precision over speed. Clarity over assumptions. Alignment over automation. Think first, ask second, code third.

Final checks:
- NEVER mention AI / Claude / ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code) / on PR, comments, commit messages
